<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>https://adapterhub.ml/blog/</id>
  <title>The AdapterHub Blog</title>
  <updated>2022-03-24T01:41:40.646755+00:00</updated>
  <link href="https://adapterhub.ml/blog/"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator>
  <subtitle>The latest news from AdapterHub</subtitle>
  <entry>
    <id>https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning</id>
    <title>Adapter-Transformers v3 - Unifying Efficient Fine-Tuning</title>
    <updated>2022-03-21T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <content type="html">&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v3_methods.png" title="Illustration of efficient fine-tuning methods supported in v3 of adapter-transformers." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of efficient fine-tuning methods supported in v3 of adapter-transformers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Since adapters were first introduced to NLP as a light-weight alternative to full fine-tuning of language models (&lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al., 2019&lt;/a&gt;), the relevance of efficient transfer learning methods has continuously gained importance throughout the field.
With Transformer-based language models growing from millions to billions or trillions of parameters, the inherent advantages of methods such as adapters - parameter efficiency, computational efficiency and modularity - have only become even more relevant.
Nowadays, the tool set of efficient fine-tuning methods contains a diverse palette of different methods, ranging from improved adapter architectures (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al., 2021&lt;/a&gt;, &lt;a href="https://aclanthology.org/2021.emnlp-main.351/"&gt;Ribeiro et al., 2021&lt;/a&gt;) to various methods of optimizing language model prompts (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;, &lt;a href="https://aclanthology.org/2021.emnlp-main.243/"&gt;Lester et al., 2021&lt;/a&gt;).
Recent work also has made attempts at combining multiple methods into a single unified architecture (&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2110.07577.pdf"&gt;Mao et al., 2021&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;With the release of version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; today, we're taking the first steps at embracing this grown and diversified landscape of efficient fine-tuning methods.
Our library, an extension of the great &lt;a href="https://huggingface.co/transformers/"&gt;Transformers library by HuggingFace&lt;/a&gt;, was introduced as a straightforward way to train, share, load and use adapters within Transformer models.
The new version for the first time allows using methods beyond the "classic" adapter architecture within this framework, namely Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters.&lt;/p&gt;
&lt;p&gt;In the following sections, we will present all new features and methods introduced with the new release as well as all important changes one by one:&lt;/p&gt;
&lt;div class="toc"&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#new-efficient-fine-tuning-methods"&gt;New Efficient Fine-Tuning Methods&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#recap-bottleneck-adapters"&gt;Recap: Bottleneck Adapters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#prefix-tuning"&gt;Prefix Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#parallel-mix-and-match-adapters"&gt;Parallel &amp;amp; Mix-and-Match adapters&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#compacters"&gt;Compacters&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#library-updates-and-changes"&gt;Library Updates and Changes&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="#xadaptermodel-classes"&gt;XAdapterModel classes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#flexible-configurations-with-configunion"&gt;Flexible configurations with ConfigUnion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#adaptersetup-context"&gt;AdapterSetup context&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#refactorings"&gt;Refactorings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#transformers-upgrade"&gt;Transformers upgrade&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#conclusion"&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#references"&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;You can find &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="new-efficient-fine-tuning-methods"&gt;New Efficient Fine-Tuning Methods&lt;/h2&gt;
&lt;p&gt;Version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; integrates a first batch of new efficient fine-tuning methods.
These include Prefix Tuning (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;), Parallel adapters, Mix-and-Match adapters (&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al., 2021&lt;/a&gt;) and Compacters (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al., 2021&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The newly added methods seamlessly integrate into the existing framework of working with adapters, i.e. they share the same methods for creation (&lt;code&gt;add_adapter()&lt;/code&gt;), training (&lt;code&gt;train_adapter()&lt;/code&gt;), saving (&lt;code&gt;save_adapter()&lt;/code&gt;) and loading (&lt;code&gt;load_adapter()&lt;/code&gt;).
Each method is specified and configured using a specific configuration class, all of which derive from the common &lt;code&gt;AdapterConfigBase&lt;/code&gt; class.
Please refer to &lt;a href="https://docs.adapterhub.ml/quickstart.html"&gt;our documentation&lt;/a&gt; for more explanation on working with adapters.&lt;/p&gt;
&lt;h3 id="recap-bottleneck-adapters"&gt;Recap: Bottleneck Adapters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
  &lt;img src="/static/images/bottleneck.png"  height="400"&gt;
  &lt;figcaption&gt;Figure 2: The bottleneck adapter network consists of a linear down projection, non-linearity, and up projection, followed by a residual connection. It 
   is positioned after the multihead attention layer and/or the feedforward layer.&lt;/figcaption&gt;
&lt;/figure&gt; 
&lt;/div&gt;

&lt;p&gt;Until version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt;, we only supported bottleneck adapters. As illustrated above, small stitched-in layers that 
consist of bottleneck feed-forward layers and a residual connection are added to the pre-trained transformer layers. These adapters are typically placed after the attention block and/or 
after the feedforward layer. For further detail check out our documentation for 
bottleneck adapters &lt;a href="https://docs.adapterhub.ml/overview"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="prefix-tuning"&gt;Prefix Tuning&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/prefix.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 3: Prefix Tuning adds trainable prefix vectors to the key and value matrices in the model.  
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Prefix Tuning (&lt;a href="https://aclanthology.org/2021.acl-long.353.pdf"&gt;Li and Liang, 2021&lt;/a&gt;) introduces new parameters in the multi-head attention blocks in each Transformer layer. 
In the illustration above the prefixes are marked pink and purple. More, specifically, we prepend trainable prefix vectors &lt;script type="math/tex"&gt;P^K&lt;/script&gt; and &lt;script type="math/tex"&gt;P^V&lt;/script&gt; to the keys and values of the attention head input, each of a configurable prefix length &lt;script type="math/tex"&gt;l&lt;/script&gt; (&lt;code&gt;prefix_length&lt;/code&gt; attribute):&lt;/p&gt;
&lt;p&gt;
&lt;script type="math/tex; mode=display"&gt;
head_i = \text{Attention}(Q W_i^Q, [P_i^K, K W_i^K], [P_i^V, V W_i^V])
&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;Following the original authors, the prefix vectors in &lt;script type="math/tex"&gt;P^K&lt;/script&gt; and &lt;script type="math/tex"&gt;P^V&lt;/script&gt; are not optimized directly, but reparameterized via a bottleneck MLP.
This behavior is controlled via the &lt;code&gt;flat&lt;/code&gt; attribute of the configuration.
Using &lt;code&gt;PrefixTuningConfig(flat=True)&lt;/code&gt; will create prefix tuning vectors that are optimized without reparameterization.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;flat&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prefix_length&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prefix_tuning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As reparameterization using the bottleneck MLP is not necessary for performing inference on an already trained Prefix Tuning module, adapter-transformers includes a function to "eject" a reparameterized Prefix Tuning into a flat one:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eject_prefix_tuning&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prefix_tuning&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will only retain the necessary parameters and reduces the size of the trained Prefix Tuning.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;The following table compares initial runs of our Prefix Tuning implementation&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;94&lt;/td&gt;
&lt;td&gt;94.72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;86.3&lt;/td&gt;
&lt;td&gt;86.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;43.40/20.46/35.51&lt;/td&gt;
&lt;td&gt;43.00/20.05/35.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;35.6&lt;/td&gt;
&lt;td&gt;35.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="parallel-mix-and-match-adapters"&gt;Parallel &amp;amp; Mix-and-Match adapters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/parallel.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 4: The parallel adapter computes representations in parallel to the transformer sublayer. It does not receive the output of 
    the attention or feedforward layer, but instead processes the same input such that the adapter is parallel to the attention or feedforward layer. The respective representations are added subsequently.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Parallel adapters have been proposed as a variant of the classic bottleneck adapter architecture.
Here, activations are passed via the bottleneck adapter layer &lt;em&gt;in parallel&lt;/em&gt; to the adapted Transformer sub-layer (i.e. feed-forward or attention layer),
as opposed to the established, sequential, order of computations.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt; study various variants and combinations of efficient fine-tuning methods.
Among others, they propose &lt;em&gt;Mix-and-Match Adapters&lt;/em&gt; as a combination of Prefix Tuning and parallel adapters.
This configuration is supported by adapter-transformers out-of-the-box:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;MAMConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MAMConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mam_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;and is identical to using the following &lt;code&gt;ConfigUnion&lt;/code&gt; (see further below for more on &lt;code&gt;ConfigUnion&lt;/code&gt;):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ParallelConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;PrefixTuningConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bottleneck_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;800&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;ParallelConfig&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mam_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Results&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;The following table compares initial runs of our Mix-and-Match adapter implementation&lt;sup id="fnref2:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;94.2&lt;/td&gt;
&lt;td&gt;94.26&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;87.4&lt;/td&gt;
&lt;td&gt;86.47&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;45.12/21.90/36.91&lt;/td&gt;
&lt;td&gt;44.74/21.75/36.80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;37.5&lt;/td&gt;
&lt;td&gt;36.9&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Additionally, the next table shows initial runs of our parallel adapter implementation, again compared with the results reported by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt; when applicable.
We use a reduction factor of 2 (corresponding to a bottleneck dimension of 384 for roberta-base and 512 for bart-large).&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;94.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;roberta-base&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;-&lt;/td&gt;
&lt;td&gt;86.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;XSum&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;R-1/R-2/R-L&lt;/td&gt;
&lt;td&gt;44.35/20.98/35.98&lt;/td&gt;
&lt;td&gt;44.88/21.53/36.55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;WMT16 En-Ro&lt;/td&gt;
&lt;td&gt;bart-large&lt;/td&gt;
&lt;td&gt;BLEU&lt;/td&gt;
&lt;td&gt;37.1&lt;/td&gt;
&lt;td&gt;36.4&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id="compacters"&gt;Compacters&lt;/h3&gt;
&lt;div align="center"&gt;
&lt;figure text-align="center"&gt;
&lt;img src="/static/images/compacter.png" height="400"&gt;
  &lt;figcaption text-align="center"&gt;
    Figure 5: The compacter replaces the linear down and up projection of the bottleneck adapter with a phm layer. 
    The phm layer obtains its weights by computing the kronecker product of two smaller matrices.
  &lt;/figcaption&gt;
 &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;Another alternative to the classical bottleneck adapter is the Compacter (&lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al. (2021)&lt;/a&gt;). Here the linear down- and up-projection layer is replaced by a phm layer, which is marked in 
black on the illustration. In the phm layer, the weights matrix is constructed from two smaller matrices by computing their kroenecker product. These matrices can be factorized and shared between all transformer layers.&lt;/p&gt;
&lt;p&gt;To add a Compacter in adapter-transformers, simply provide a &lt;code&gt;CompacterConfig&lt;/code&gt;or a &lt;code&gt;CompacterPlusPlusConfig&lt;/code&gt; when adding the adapter:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CompacterPlusPlusConfig&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;CompacterPlusPlusConfig&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;compacter_plusplus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following table compares the results of training a Compacter++&lt;sup id="fnref3:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; for T5 for the glue tasks with the results reported in &lt;a href="https://arxiv.org/pdf/2106.04647.pdf"&gt;Mahabadi et al. (2021)&lt;/a&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Task&lt;/th&gt;
&lt;th&gt;Metrics&lt;/th&gt;
&lt;th&gt;Reference&lt;/th&gt;
&lt;th&gt;Ours&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;COLA&lt;/td&gt;
&lt;td&gt;Mathews Correlation&lt;/td&gt;
&lt;td&gt;61.27&lt;/td&gt;
&lt;td&gt;58.45&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;SST-2&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;93.81&lt;/td&gt;
&lt;td&gt;94.61&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MRPC&lt;/td&gt;
&lt;td&gt;Acc./F1&lt;/td&gt;
&lt;td&gt;90.69/93.33&lt;/td&gt;
&lt;td&gt;87.99/91.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QQP&lt;/td&gt;
&lt;td&gt;Acc./F1&lt;/td&gt;
&lt;td&gt;90.17/86.93&lt;/td&gt;
&lt;td&gt;90.33/87.46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;STS-B&lt;/td&gt;
&lt;td&gt;Pearson/Spearman Correlation&lt;/td&gt;
&lt;td&gt;90.46/90.93&lt;/td&gt;
&lt;td&gt;89.78/89.53&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MNLI&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;85.71&lt;/td&gt;
&lt;td&gt;85.32&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QNLI&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;93.08&lt;/td&gt;
&lt;td&gt;91.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;RTE&lt;/td&gt;
&lt;td&gt;Acc.&lt;/td&gt;
&lt;td&gt;74.82&lt;/td&gt;
&lt;td&gt;77.25&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="library-updates-and-changes"&gt;Library Updates and Changes&lt;/h2&gt;
&lt;p&gt;Below, we highlight further updates and changes introduced with v3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt;.
You can find a full change log &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/releases/tag/adapters3.0.0"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="xadaptermodel-classes"&gt;&lt;code&gt;XAdapterModel&lt;/code&gt; classes&lt;/h3&gt;
&lt;p&gt;Version 3.0 introduces a new set of model classes (one class per model type) specifically designed for working with adapters.
These classes follow the general schema &lt;code&gt;XAdapterModel&lt;/code&gt;, where &lt;code&gt;X&lt;/code&gt; is the respective model type (e.g. &lt;code&gt;Bert&lt;/code&gt;, &lt;code&gt;GPT2&lt;/code&gt;).
They replace the &lt;code&gt;XModelWithHeads&lt;/code&gt; classes of earlier versions.
In summary, these classes provide the following main features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Flexible configuration of predictions heads (see &lt;a href="https://docs.adapterhub.ml/prediction_heads.html#adaptermodel-classes"&gt;documentation&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Compositions (such as parallel inference and &lt;code&gt;BatchSplit&lt;/code&gt;) of adapters with different prediction heads.&lt;/li&gt;
&lt;li&gt;One model class per model type, additionally, a &lt;code&gt;AutoAdapterModel&lt;/code&gt; class for automatic class detection.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;These classes are designed as the new default classes of &lt;code&gt;adapter-transformers&lt;/code&gt;. It is recommended to use these classes for working with adapters whenever possible.&lt;/strong&gt;
A usage example looks like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;

&lt;span class="c1"&gt;# Load class&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoAdapterModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Configure adapters &amp;amp; heads&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_multiple_choice_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_choices&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define active setup&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Parallel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;first_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;second_task&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Start training loop ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;⚠️ All &lt;code&gt;XModelWithHeads&lt;/code&gt; classes are now deprecated as the new classes are direct replacements.&lt;/p&gt;
&lt;h3 id="flexible-configurations-with-configunion"&gt;Flexible configurations with &lt;code&gt;ConfigUnion&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;While different efficient fine-tuning methods and configurations have often been proposed as standalone, it might be beneficial to combine them for joint training.
We have already seen this for the &lt;em&gt;Mix-and-Match&lt;/em&gt; adapters proposed by &lt;a href="https://arxiv.org/pdf/2110.04366.pdf"&gt;He et al. (2021)&lt;/a&gt;.
To make this process easier, adapter-transformers provides the possibility to group multiple configuration instances together using the &lt;code&gt;ConfigUnion&lt;/code&gt; class.&lt;/p&gt;
&lt;p&gt;For example, this could be used to define different reduction factors for the adapter modules placed after the multi-head attention and the feed-forward blocks:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;

&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ConfigUnion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mh_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;non_linearity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;AdapterConfig&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mh_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output_adapter&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction_factor&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;non_linearity&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;union_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="adaptersetup-context"&gt;&lt;code&gt;AdapterSetup&lt;/code&gt; context&lt;/h3&gt;
&lt;p&gt;As a replacement to the &lt;code&gt;adapter_names&lt;/code&gt; parameter, v3.0 introduces a new &lt;code&gt;AdapterSetup&lt;/code&gt; class for dynamic and state-less configuration of activated adapters.
This class is intended to be used as a context manager, i.e. a typical use case would look like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# will use no adapters&lt;/span&gt;
&lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="c1"&gt;# will use the adapter stack &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that in the above example &lt;strong&gt;no&lt;/strong&gt; adapters are activated via &lt;code&gt;active_adapters&lt;/code&gt;. Within the &lt;code&gt;with&lt;/code&gt; block, the adapter implementation will dynamically read the currently active setup from the context manager.&lt;/p&gt;
&lt;p&gt;This solution allows dynamic adapter activation, e.g. also with nesting:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="c1"&gt;# will use the adapter stack &amp;quot;a&amp;quot; and &amp;quot;b&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;AdapterSetup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Fuse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;head_setup&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;e&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# will use fusion between &amp;quot;c&amp;quot; and &amp;quot;d&amp;quot; &amp;amp; head &amp;quot;e&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;inputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Most importantly, the context manager is &lt;strong&gt;thread-local&lt;/strong&gt;, i.e. we can use different setups in different threads with the same model instance.&lt;/p&gt;
&lt;p&gt;⚠️ Breaking change: The &lt;code&gt;adapter_names&lt;/code&gt; parameter is removed for all model classes.&lt;/p&gt;
&lt;h3 id="refactorings"&gt;Refactorings&lt;/h3&gt;
&lt;p&gt;Besides the already mentioned changes, v3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; comes with major refactorings in the integration of adapter implementations into model classes and model configurations (e.g., see &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/pull/263"&gt;here&lt;/a&gt; and &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/pull/304"&gt;here&lt;/a&gt;).
While these refactorings only affect the interface methods minimally, the process of integrating new model architectures has been substantially simplified.
Please refer to the &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/blob/master/adding_adapters_to_a_model.md"&gt;updated model integration guide&lt;/a&gt; for more.&lt;/p&gt;
&lt;h3 id="transformers-upgrade"&gt;Transformers upgrade&lt;/h3&gt;
&lt;p&gt;Version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; upgrades the underlying HuggingFace Transformers library from v4.12.5 to v4.17.0, bringing many awesome new features created by HuggingFace.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The release of version 3.0 of &lt;code&gt;adapter-transformers&lt;/code&gt; today marks the starting point of integrating new efficient fine-tuning methods.
In this release, we integrated a first batch of recently proposed methods, including Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters.
Nonetheless, the range of available efficient fine-tuning methods goes far beyond these and continues to grow rapidly.
Thus, we expect to integrate more and more methods step by step.&lt;/p&gt;
&lt;p&gt;Finally, as we're a very small team, your help on &lt;code&gt;adapter-transformers&lt;/code&gt; is always very welcome.
Head over to our &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;GitHub repository&lt;/a&gt; and reach out if you're interested in contributing in any way.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019.&lt;/li&gt;
&lt;li&gt;Mahabadi, R.K., Henderson, J., &amp;amp; Ruder, S. (2021). Compacter: Efficient Low-Rank Hypercomplex Adapter Layers. ArXiv, abs/2106.04647.&lt;/li&gt;
&lt;li&gt;Leonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych. 2021. Structural Adapters in Pretrained Language Models for AMR-to-Text Generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4269–4282, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, Online. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.&lt;/li&gt;
&lt;li&gt;He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., &amp;amp; Neubig, G. (2021). Towards a Unified View of Parameter-Efficient Transfer Learning. ArXiv, abs/2110.04366.&lt;/li&gt;
&lt;li&gt;Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., Yih, W., &amp;amp; Khabsa, M. (2021). UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning. ArXiv, abs/2110.07577.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Reported results for &lt;code&gt;adapter-transformers&lt;/code&gt; only contain a single run each without hyperparameter tuning.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content>
    <link href="https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning" rel="alternate"/>
    <summary>With the release of version 3.0 of adapter-transformers today, we're taking the first steps at integrating the grown and diversified landscape of efficient fine-tuning methods. Version 3.0 adds support for a first batch of recently proposed methods, including Prefix Tuning, Parallel adapters, Mix-and-Match adapters and Compacters. Further, improvements and changes to various aspects of the library are introduced.</summary>
    <published>2022-03-21T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub</id>
    <title>Adapting Transformers with AdapterHub</title>
    <updated>2020-11-17T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <content type="html">&lt;p&gt;Transformer models pre-trained on massive amounts of text data and subsequently fine-tuned on target tasks have led to considerable advances in NLP, achieving state-of-the-art results across the board. 
However, models such as BERT (&lt;a href="https://arxiv.org/pdf/1810.04805.pdf"&gt;Devlin et al., 2019&lt;/a&gt;) and RoBERTa (&lt;a href="https://arxiv.org/pdf/1907.11692.pdf"&gt;Liu et al., 2019&lt;/a&gt;) consist of several millions of parameters, and thus, sharing and distributing fully fine-tuned models for each individual downstream task can be prohibitive. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adapters&lt;/strong&gt; are a light-weight alternative to full model fine-tuning, consisting of only a tiny set of newly introduced parameters at every transformer layer.
Adapters overcome several limitations typically observed with full model fine-tuning:
they are &lt;strong&gt;parameter-efficient&lt;/strong&gt;, they &lt;strong&gt;speed up training iterations&lt;/strong&gt;, and they are &lt;strong&gt;shareable&lt;/strong&gt; and &lt;strong&gt;composable&lt;/strong&gt; due to their modularity and compact size.
Moreover, adapters usually perform &lt;strong&gt;on-par with state-of-the-art full fine-tuning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With multiple different adapter architectures and a wide variety of pre-trained transformers available, training, sharing and re-using adapters is not straightforward.
As a solution,  &lt;strong&gt;&lt;a href="https://arxiv.org/pdf/2007.07779.pdf"&gt;AdapterHub, A Framework for Adapting Transformers&lt;/a&gt;&lt;/strong&gt; provides a unified interface to different adapter architectures and composition techniques, making them widely accessible to the research community.
Built on top of &lt;a href="https://github.com/huggingface/transformers"&gt;Huggingface's Transformers framework&lt;/a&gt;, the AdapterHub has access to a large base of pre-trained transformers.
In the following, we will go through the process of training, sharing, and composing adapters with AdapterHub.&lt;/p&gt;
&lt;h2 id="a-short-introduction-to-adapters"&gt;A Short Introduction to Adapters&lt;/h2&gt;
&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/steps.gif" title="Steps of working with adapters" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Steps of working with adapters&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Adapters provide a lightweight alternative to fully fine-tuning a pre-trained language model on a downstream task.
For a transformer-based architecture, a small set of new parameters is introduced in every transformer layer.
While different adapter architectures are possible, a simple layout using a down- and an up-projection layer first introduced by &lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al. (2020)&lt;/a&gt; has proven to work well (see Figure 1 for illustration).
In many cases, adapters perform on-par with fully fine-tuned models.&lt;/p&gt;
&lt;p&gt;During training on the target task, all weights of the pre-trained language model are kept fix.
The only weights to be updated are those introduced by the adapter modules.
This results in modular knowledge representations which subsequently can be easily extracted from the underlying language model.
The extracted adapter modules then can be distributed independently and plugged into a language model dynamically.
The encapsulated character of adapters also allows for easy exchange and composition of different adapters (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2020a&lt;/a&gt;).
Since this workflow of using adapters is very universal, it can potentially be applied to a wide range of different use cases.
As an example, adapters have been used successfully for zero-shot cross-lingual transfer between different tasks (&lt;a href="https://arxiv.org/pdf/2005.00052.pdf"&gt;Pfeiffer et al., 2020b&lt;/a&gt;).
Figure 1 illustrates the described adapter workflow.&lt;/p&gt;
&lt;figure id="_caption-2"&gt;
&lt;img alt="" src="/static/images/size_comparison.png" title="Size comparison of a fully fine-tuned model and an adapter" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;2:&lt;/span&gt; Size comparison of a fully fine-tuned model and an adapter&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Using adapters provides various benefits, especially in parameter efficiency.
The amount of updated adapter parameters are only about 1% of the fully fine-tuned model and in many cases only requires a few Megabytes of storage space.
This makes it easy to share adapters, store adapters for many different tasks and load additional adapters on-the-fly.
Additionally, their compact size with the majority of weights bein frozen, makes adapters a computationally efficient fine-tuning choice (&lt;a href="https://arxiv.org/pdf/2010.11918.pdf"&gt;Rücklé et al., 2020&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id="what-is-adapterhub"&gt;What is AdapterHub?&lt;/h2&gt;
&lt;p&gt;With AdapterHub, we have developed a framework which makes working with adapters straightforward.
AdapterHub is divided into two core components: &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;adapter-transformers&lt;/a&gt;, a library built on top of HuggingFace &lt;code&gt;transformers&lt;/code&gt; that integrates adapter support into various popular Transformer-based language models, and &lt;a href="https://adapterhun.ml/explore"&gt;the Hub&lt;/a&gt;, an open platform for sharing, exploring and consuming pre-trained adapters.&lt;/p&gt;
&lt;figure id="_caption-3"&gt;
&lt;img alt="" src="/static/images/lifecycle.png" title="The AdapterHub lifecycle" /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;3:&lt;/span&gt; The AdapterHub lifecycle&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Based on Figure 3, we'll go through the lifecycle of working with AdapterHub on a higher level:
HuggingFace &lt;code&gt;transformers&lt;/code&gt; (🤗) builds the backbone of our framework.
A user who wants to train an adapter (👩🏾‍💻) loads a pre-trained language model (🤖) from 🤗.
In ①, new adapter modules are introduced to the loaded language model.
Afterwards, 👩🏾‍💻 trains the adapter on a downstream task (②).
As soon as training has completed, 👩🏾‍💻 can extract the trained adapter weights from the (unaltered) 🤖 in ③.
👩🏾‍💻 packs the adapter weights and uploads them to the Hub.
Here, 👨🏼‍💻 can find the pre-trained adapter in step ④.
Together with downloading the matching 🤖 from 🤗, 👨🏼‍💻 then can download the adapter from the Hub and integrate it into his own model (⑤).
In ⑥, he lastly can apply 👩🏾‍💻's adapter for his own purposes.&lt;/p&gt;
&lt;p&gt;In the following, we will have a look at some of these steps in more detail.&lt;/p&gt;
&lt;h2 id="training-an-adapter"&gt;Training an Adapter&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Training an adapter on a downstream task is a straightforward process using &lt;code&gt;adapter-transformers&lt;/code&gt; which can be installed via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This package is fully compatible with HuggingFace's &lt;code&gt;transformers&lt;/code&gt; library and can act as a drop-in replacement. Therefore, we can instantiate a pre-trained language model and tokenizer in the familiar way:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RobertaTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RobertaConfig&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;RobertaModelWithHeads&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;config&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaConfig&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;id2label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;👎&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;👍&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RobertaModelWithHeads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;roberta-base&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There is one difference compared to HuggingFace transformers in the code above:
We use the new class &lt;code&gt;RobertaModelWithHeads&lt;/code&gt; which allows a more flexible way of configuring prediction heads.&lt;/p&gt;
&lt;p&gt;The next steps configure our adapter setup. Note that these are the only lines additionally needed to switch from full fine-tuning to adapter training.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;

&lt;span class="c1"&gt;# Add a new adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a matching classification head&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Activate the adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We add a new adapter to our model by calling &lt;code&gt;add_adapter()&lt;/code&gt;. We pass a name (&lt;code&gt;"rotten_tomatoes"&lt;/code&gt;) and &lt;a href="https://docs.adapterhub.ml/adapters.html#adapter-types"&gt;the type of adapter&lt;/a&gt; (task adapter). Next, we add a binary classification head. It's convenient to give the prediction head the same name as the adapter. This allows us to activate both together in the next step. The &lt;code&gt;train_adapter()&lt;/code&gt; method does two things:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It freezes all weights of the pre-trained model so only the adapter weights are updated during training.&lt;/li&gt;
&lt;li&gt;It activates the adapter and the prediction head such that both are used in every forward pass.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;All the rest of the training process is identical to a full fine-tuning approach. Check out &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/01_Adapter_Training.ipynb"&gt;the Colab notebook on adapter training&lt;/a&gt; to see the full code.&lt;/p&gt;
&lt;p&gt;In the end, the trained adapter can be exported to the file system using a single line of code:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;./final_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rotten_tomatoes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="interacting-with-the-hub"&gt;Interacting with the Hub&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The adapter weights trained in the previous section subsequently can be distributed via the Hub, the second core component of the AdapterHub framework.
The Hub infrastructure is based on plain YAML description files contributed to a central GitHub repository.
The full process of contributing pre-trained adapters is described &lt;a href="https://docs.adapterhub.ml/contributing.html"&gt;in our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://adapterhub.ml/explore"&gt;Explore section&lt;/a&gt; of the AdapterHub website acts as the starting point for discovering and consuming available pre-trained adapters. A matching adapter can be selected by task domain, training dataset, model architecture and adapter architecture and loaded into &lt;code&gt;adapter-transformers&lt;/code&gt; in the following.&lt;/p&gt;
&lt;p&gt;Before loading the adapter, we instantiate the model we want to use, a pre-trained &lt;code&gt;bert-base-uncased&lt;/code&gt; model from HuggingFace.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AutoModelWithHeads&lt;/span&gt;

&lt;span class="n"&gt;tokenizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoTokenizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelWithHeads&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using &lt;code&gt;load_adapter()&lt;/code&gt;, we download and add a pre-trained adapter from the Hub. The first parameter specifies the name of the adapter whereas the second selects the &lt;a href="https://docs.adapterhub.ml/adapters.html#adapter-architectures"&gt;adapter architectures&lt;/a&gt; to search for.&lt;/p&gt;
&lt;p&gt;Also note that most adapters come with a prediction head included. Thus, this method will also load the question answering head trained together with the adapter.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa/squad1@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code&gt;set_active_adapters()&lt;/code&gt; we tell our model to use the adapter we just loaded in every forward pass.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Again, these are all changes needed to set up a pre-trained language model with a pre-trained adapter.
The rest of the inference is identical to a setup without adapters.
To see a full example, check out &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/02_Adapter_Inference.ipynb"&gt;the Colab notebook for adapter inference&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="adapter-composition"&gt;Adapter Composition&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb" target="_blank"&gt;
&lt;img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As presented earlier, adapters are especially suitable for various kinds of compositions on a new target task.
One of these composition approaches is &lt;em&gt;AdapterFusion&lt;/em&gt; (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2020&lt;/a&gt;) which is also tightly integrated into AdapterHub.&lt;/p&gt;
&lt;p&gt;The knowledge learned by multiple pre-trained adapters from the Hub can be leveraged to solve a new target task.
In this setup, only a newly introduced fusion layer is trained while the rest of the model is kept fix.&lt;/p&gt;
&lt;p&gt;First, we load three adapters pre-trained on different tasks from the Hub: MultiNLI, QQP and QNLI. As we don't need their prediction heads, we pass &lt;code&gt;with_head=False&lt;/code&gt; to the loading method. Next, we add a new fusion layer that combines all the adapters we've just loaded. Finally, we add a new classification head for our target task on top.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;

&lt;span class="c1"&gt;# Load the pre-trained adapters we want to fuse&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nli/multinli@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;load_as&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sts/qqp@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nli/qnli@ukp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;with_head&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a fusion layer for all loaded adapters&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_fusion&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qqp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qnli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Add a classification head for our target task&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_classification_head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_labels&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;id2label&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The last preparation step is to define and activate our adapter setup. Similar to &lt;code&gt;train_adapter()&lt;/code&gt;, &lt;code&gt;train_fusion()&lt;/code&gt; does two things: It freezes all weights of the model (including adapters!) except for the fusion layer and classification head. It also activates the given adapter setup to be used in very forward pass.&lt;/p&gt;
&lt;p&gt;The syntax for the adapter setup (which is also applied to other methods such as &lt;code&gt;set_active_adapters()&lt;/code&gt;) works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a single string is interpreted as a single adapter&lt;/li&gt;
&lt;li&gt;a list of strings is interpreted as a &lt;strong&gt;stack&lt;/strong&gt; of adapters&lt;/li&gt;
&lt;li&gt;a &lt;em&gt;nested&lt;/em&gt; list of strings is interpreted as a &lt;strong&gt;fusion&lt;/strong&gt; of adapters&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Unfreeze and activate fusion setup&lt;/span&gt;
&lt;span class="n"&gt;adapter_setup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
  &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;multinli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qqp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;qnli&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_fusion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;adapter_setup&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See the full training example in &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/03_Adapter_Fusion.ipynb"&gt;the Colab notebook on AdapterFusion&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Adapters are a promising new approach to transfer learning in NLP, providing benefits in efficiency and modularity.
AdapterHub provides tools for the full lifecycle of interacting with adapters.
The integration into the successful HuggingFace &lt;code&gt;transformers&lt;/code&gt; framework makes it straightforward to adapt training setups to adapters.
AdapterHub is continuously evolving with the addition of adapter support to new models, the integration of new application scenarios for adapters and a growing platform of pre-trained adapter modules.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Devlin, J., Chang, M., Lee, K., &amp;amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT.&lt;/li&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML.&lt;/li&gt;
&lt;li&gt;Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;amp; Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv, abs/1907.11692.&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2020). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. ArXiv, abs/2005.00247.&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Vulic, I., Gurevych, I., &amp;amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. ArXiv, abs/2005.00052.&lt;/li&gt;
&lt;li&gt;Rücklé, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., &amp;amp; Gurevych, I. (2020). AdapterDrop: On the Efficiency of Adapters in Transformers. ArXiv, abs/2010.11918.&lt;/li&gt;
&lt;li&gt;Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., &amp;amp; Brew, J. (2019). HuggingFace's Transformers: State-of-the-art Natural Language Processing. ArXiv, abs/1910.03771.&lt;/li&gt;
&lt;/ul&gt;</content>
    <link href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub" rel="alternate"/>
    <summary>Adapters are a new, efficient and composable alternative to full fine-tuning of pre-trained language models.
AdapterHub makes working with adapters accessible by providing a framework for training, sharing, discovering and consuming adapter modules.
This post provides an extensive overview.
</summary>
    <published>2020-11-17T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released</id>
    <title>Version 2 of AdapterHub Released</title>
    <updated>2021-04-29T00:00:00+00:00</updated>
    <author>
      <name>Clifton Poth</name>
    </author>
    <author>
      <name>Hannah Sterz</name>
    </author>
    <content type="html">&lt;figure id="_caption-1"&gt;
&lt;img alt="" src="/static/images/v2_blocks.png" title="Illustration of adapter composition blocks supported in v2 of adapter-transformers." /&gt;
&lt;figcaption&gt;&lt;span&gt;Figure&amp;nbsp;1:&lt;/span&gt; Illustration of adapter composition blocks supported in v2 of adapter-transformers.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Adapters, a light-weight alternative to full language model fine-tuning, enable new ways of composing task-specific knowledge from multiple sources, e.g., for multi-task transfer learning (&lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al., 2021&lt;/a&gt;) or cross-lingual transfer (&lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.617.pdf"&gt;Pfeiffer et al., 2020&lt;/a&gt;).
One of the most important advantages of adapters is their modularity, which allows many exciting possibilities for composition beyond the ones mentioned above.&lt;/p&gt;
&lt;p&gt;Today, we are releasing Version 2 of the &lt;a href="https://adapterhub.ml/"&gt;AdapterHub framework&lt;/a&gt;, including a major update of &lt;code&gt;adapter-transformers&lt;/code&gt;, which makes it easier to take advantage of the composability and flexibility of adapters.
&lt;code&gt;adapter-transformers&lt;/code&gt; --- an extension of the great &lt;a href="https://huggingface.co/transformers/"&gt;Transformers library by HuggingFace&lt;/a&gt; --- is the heart of the AdapterHub that simplifies the entire adapter lifecycle.
(Check out &lt;a href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/"&gt;our first blog post for more on this&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;In the following sections, we will discuss all new features and changes that we introduce with the v2 release.
You can find &lt;code&gt;adapter-transformers&lt;/code&gt; &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt; or install it via pip:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install -U adapter-transformers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="whats-new"&gt;What's new&lt;/h2&gt;
&lt;h3 id="adapter-composition-blocks"&gt;Adapter composition blocks&lt;/h3&gt;
&lt;p&gt;The new version introduces a radically different way to define adapter setups in a Transformer model,
allowing much more advanced and flexible adapter composition possibilities.
An example setup using this new, modular composition mechanism might look like this:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;transformers.adapters.composition&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;ac&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ac&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;c&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As we can see, the basic building blocks of this setup are simple objects representing different possibilities to combine individual adapters.
In the above example, &lt;code&gt;Stack&lt;/code&gt; describes stacking adapters layers on top of each other,
e.g., as it is used in the &lt;em&gt;MAD-X&lt;/em&gt; framework for cross-lingual transfer.
&lt;code&gt;Split&lt;/code&gt; results in splitting the input sequences between two adapters at a specified &lt;code&gt;split_index&lt;/code&gt;.
In the depicted setup, at every transformer layer the token representations are first passed through adapter &lt;code&gt;a&lt;/code&gt; before being split at the &lt;code&gt;split_index&lt;/code&gt; and passed through adapters &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; respectively.&lt;/p&gt;
&lt;p&gt;Besides the two blocks shown, &lt;code&gt;adapter-transformers&lt;/code&gt; includes a &lt;code&gt;Fuse&lt;/code&gt; block (for &lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;&lt;em&gt;AdapterFusion&lt;/em&gt;&lt;/a&gt;) and a &lt;code&gt;Parallel&lt;/code&gt; block (see below).
All of these blocks are derived from &lt;code&gt;AdapterCompositionBlock&lt;/code&gt;, and they can be flexibly combined in even very complex scenarios.
Figure 1 shows an illustration of the structure of each composition block.
For more information on specifying the active adapters using &lt;code&gt;active_adapters&lt;/code&gt; and the new composition blocks,
refer to the &lt;a href="https://docs.adapterhub.ml/adapter_composition.html"&gt;corresponding section in our documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="new-model-support-adapters-for-bart-and-gpt-2"&gt;New model support: Adapters for BART and GPT-2&lt;/h3&gt;
&lt;p&gt;Version 2 adds support for BART and GPT-2, marking a new type of models we support in the framework, namely sequence-to-sequence models (more to come!)&lt;/p&gt;
&lt;p&gt;We have &lt;a href="https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/"&gt;a separate blog post&lt;/a&gt; that studies the effectiveness of adapters within these two models in greater detail! This blog post also includes a hands-on example where we train GPT-2 to generate poetry.&lt;/p&gt;
&lt;h3 id="adapterdrop"&gt;AdapterDrop&lt;/h3&gt;
&lt;p&gt;Version 2 of &lt;code&gt;adapter-transformers&lt;/code&gt; integrates some of the key ideas presented in &lt;em&gt;AdapterDrop&lt;/em&gt; &lt;a href="https://arxiv.org/pdf/2010.11918.pdf"&gt;(Rücklé et al., 2020)&lt;/a&gt;, namely, (1) parallel multi-task inference and (2) &lt;em&gt;robust&lt;/em&gt; AdapterDrop training. &lt;/p&gt;
&lt;p&gt;Parallel multi-task inference, for any given input, runs multiple task adapters in parallel and thereby achieves considerable improvements in inference speed compared to sequentially running multiple Transformer models (see the paper for more details). The &lt;code&gt;Parallel&lt;/code&gt; adapter composition block implements this behavior, which we describe in more detail &lt;a href="adapter_composition.html#parallel"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A central advantage of multi-task inference is that it shares the computations in lower transformer layers across all inference tasks (before the first adapter block). Dropping out adapters from lower transformer layers can thus result in even faster inference speeds, but it often comes at the cost of lower accuracies. To allow for &lt;em&gt;dynamic&lt;/em&gt; adjustment of the number of dropped adapter layers at run-time regarding the available computational resources, we introduce &lt;em&gt;robust&lt;/em&gt; adapter training. This technique drops adapters from a random number of lower transformer layers in each training step. The resulting adapter can be adjusted at run-time regarding the number of dropped layers, to dynamically select between a higher accuracy or faster inference speeds.
We present an example for robust &lt;em&gt;AdapterDrop&lt;/em&gt; training &lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/05_Adapter_Drop_Training.ipynb"&gt;in this Colab notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id="transformers-upgrade"&gt;Transformers upgrade&lt;/h3&gt;
&lt;p&gt;Version 2.0.0 upgrades the underlying HuggingFace Transformers library from v3.5.1 to v4.5.1, bringing many awesome new features created by HuggingFace.&lt;/p&gt;
&lt;h2 id="what-has-changed"&gt;What has changed&lt;/h2&gt;
&lt;h3 id="unified-handling-of-all-adapter-types"&gt;Unified handling of all adapter types&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Includes breaking changes ⚠️&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The new version removes the hard distinction between &lt;em&gt;task&lt;/em&gt; and &lt;em&gt;language&lt;/em&gt; adapters (realized using the &lt;code&gt;AdapterType&lt;/code&gt; enumeration in v1) everywhere in the library.
Instead, all adapters use the same set of methods.
This results in some breaking changes.
For example, you don't have to specify the adapter type anymore when adding a new adapter.
Instead of...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... you would simply write...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;houlsby&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A similar change applies for loading adapters from the Hub using &lt;code&gt;load_adapter()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In v1, adapters of type &lt;code&gt;text_lang&lt;/code&gt; automatically had invertible adapter modules added.
As this type distinction is now removed, adding invertible adapters can be specified via the adapter config.
For example...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;AdapterType&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text_task&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pfeiffer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... in v1 would be equivalent to the following in v2:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pfeiffer+inv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id="changes-to-adapter_names-parameter"&gt;Changes to &lt;code&gt;adapter_names&lt;/code&gt; parameter&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Version 2.0.0 temporarily removed the &lt;code&gt;adapter_names&lt;/code&gt; parameter entirely. Due to user feedback, it was re-added in v2.0.1.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One possibility to specify the active adapters is to use the &lt;code&gt;adapter_names&lt;/code&gt; parameter in each call to the model's &lt;code&gt;forward()&lt;/code&gt; method.
With the integration of the new, unified mechanism for specifying adapter setups using composition blocks,
it is now recommended to specify the active adapters via &lt;code&gt;set_active_adapters()&lt;/code&gt; or the &lt;code&gt;active_adapters&lt;/code&gt; property.
For example...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# OLD (v1)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;adapter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;awesome_adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;... would become...&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# NEW (v2)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;active_adapters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;awesome_adapter&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;input_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id="internal-changes"&gt;Internal changes&lt;/h2&gt;
&lt;h3 id="changes-to-adapter-weights-dictionaries-and-config"&gt;Changes to adapter weights dictionaries and config&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Includes breaking changes ⚠️&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;With the unification of different adapter types and other internal refactorings, the names of the modules holding the adapters have changed.
This affects the weights dictionaries exported by &lt;code&gt;save_adapter()&lt;/code&gt;, making the adapters incompatible &lt;em&gt;in name&lt;/em&gt;.
Nonetheless, this does not visibly affect loading older adapters with the new version.
When loading an adapter trained with v1 in a newer version, &lt;code&gt;adapter-transformers&lt;/code&gt; will automatically convert the weights to the new format.
However, loading adapters trained with newer versions into an earlier v1.x version of the library does not work.&lt;/p&gt;
&lt;p&gt;Additionally, there have been some changes in the saved configuration dictionary, also including automatic conversions from older versions.&lt;/p&gt;
&lt;h3 id="refactorings-in-adapter-implementations"&gt;Refactorings in adapter implementations&lt;/h3&gt;
&lt;p&gt;There have been some refactorings mainly in the adapter mixin implementations.
Most importantly, all adapter-related code has been moved to the &lt;code&gt;transformers.adapters&lt;/code&gt; namespace.
Further details on the implementation can be found &lt;a href="https://github.com/Adapter-Hub/adapter-transformers/blob/master/adding_adapters_to_a_model.md"&gt;in the guide for adding adapters to a new model&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;As part of the new AdapterHub release, version 2 of &lt;code&gt;adapter-transformers&lt;/code&gt; brings a range of new features to broaden the possibilities of working with adapters.
The library is still under active development, so make sure to check it out &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;on GitHub&lt;/a&gt;.
Also, we're always happy for any kind of contributions!&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019, &lt;a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf"&gt;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Rücklé, A., Poth, C., Kamath, A., Vulić, I., Ruder, S., Cho, K., &amp;amp; Gurevych, I. (2020). AdapterHub: A Framework for Adapting Transformers. EMNLP 2020 &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf"&gt;https://www.aclweb.org/anthology/2020.emnlp-demos.7.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2021). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. EACL 2021, &lt;a href="https://www.aclweb.org/anthology/2021.eacl-main.39.pdf"&gt;https://www.aclweb.org/anthology/2021.eacl-main.39.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Vulic, I., Gurevych, I., &amp;amp; Ruder, S. (2020). MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.617/"&gt;https://www.aclweb.org/anthology/2020.emnlp-main.617/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rücklé, A., Geigle, G., Glockner, M., Beck, T., Pfeiffer, J., Reimers, N., &amp;amp; Gurevych, I. (2020). AdapterDrop: On the Efficiency of Adapters in Transformers. ArXiv, &lt;a href="https://arxiv.org/abs/2010.11918"&gt;https://arxiv.org/abs/2010.11918&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., &amp;amp; Brew, J. (2019). Transformers: State-of-the-Art Natural Language Processing. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-demos.6/"&gt;https://www.aclweb.org/anthology/2020.emnlp-demos.6/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>
    <link href="https://adapterhub.ml/blog/2021/04/version-2-of-adapterhub-released" rel="alternate"/>
    <summary>Today, we are releasing version 2 of the AdapterHub. This release introduces several exciting new ways for composing adapters through composition blocks, including AdapterFusion, parallel inference, Adapter stacking, and combinations thereof. Furthermore, we now support new Transformer architectures such as GPT-2 and BART.</summary>
    <published>2021-04-29T00:00:00+00:00</published>
  </entry>
  <entry>
    <id>https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp</id>
    <title>Adapters for Generative and Seq2Seq Models in NLP</title>
    <updated>2021-04-29T00:00:00+00:00</updated>
    <author>
      <name>Hannah Sterz*</name>
    </author>
    <author>
      <name>Clifton Poth*</name>
    </author>
    <author>
      <name>Andreas Rücklé</name>
    </author>
    <author>
      <name>Jonas Pfeiffer</name>
    </author>
    <content type="html">&lt;p align="center"&gt;
&lt;img src="/static/images/BARTLogo.png"&gt;
&lt;/p&gt;

&lt;p&gt;Adapters are becoming more and more important in machine learning for NLP. For instance, they enable us to efficiently train and share new task-specific models. Adapters are small layers that are stitched into pre-trained transformer-based models. During training, only the parameters of the adapter layers are finetuned, while the parameters of the pre-trained model remain frozen. As a result, it is sufficient to only store the adapter layers instead of storing fully finetuned models separately for each task. Furthermore, the lower number of parameters requires less memory and makes it easier to share the trained adapters. Adapters also enable new possibilities in transfer learning. As adapters are encapsulated between frozen layers, they can be regarded as modular units which can be composed in a number of different ways (For more details and examples check out &lt;a href="https://adapterhub.ml/blog/2020/11/adapting-transformers-with-adapterhub/"&gt;this blog post&lt;/a&gt;). &lt;a href="https://www.aclweb.org/anthology/D19-1165.pdf"&gt;Bapna et al. (2019)&lt;/a&gt; have shown that adapters are useful for sequence to sequence tasks. On a neural machine translation task, they achieved similar results with adapters as compared to a fully finetuned model. The modularity aspect of adapters in zero-shot machine translation has recently been demonstrated by &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf"&gt;Philip et al. (2020)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The AdapterHub framework makes adapters easy to use. Up until now, the framework included adapters for the models BERT, RoBERTa, XML-RoBERTa and DistilBERT. In the new version 2.0, the framework now also provides adapters for the language generation models BART and GPT-2. This will allow researchers and engineers to use adapters for sequence-to-sequence tasks.&lt;/p&gt;
&lt;h2 id="results-of-bart-and-gpt-2-with-adapters"&gt;Results of BART and GPT-2 with adapters&lt;/h2&gt;
&lt;p&gt;Before we dive into generation tasks, we will take a look at the performance on the GLUE benchmark. We compare the scores of a fully finetuned model with the scores of adapter-based models, either using the adapter configuration of &lt;a href="https://arxiv.org/pdf/2005.00247.pdf"&gt;Pfeiffer et al. (2020a)&lt;/a&gt; or &lt;a href="https://arxiv.org/pdf/1902.00751.pdf"&gt;Houlsby et al. (2020)&lt;/a&gt;. The GPT-2 model and BART models achieve the following scores:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt; GPT-2 &lt;/th&gt;&lt;th&gt; Full &lt;/th&gt;&lt;th&gt; Pfeiffer &lt;/th&gt;&lt;th&gt; Houlsby &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; RTE &lt;/td&gt;&lt;td&gt; 65.0 &lt;/td&gt;&lt;td&gt; 67.1 &lt;/td&gt;&lt;td&gt; 67.5 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MRPC  &lt;/td&gt;&lt;td&gt; 83.8 &lt;/td&gt;&lt;td&gt; 83.5 &lt;/td&gt;&lt;td&gt; 80.4 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   STS-B &lt;/td&gt;&lt;td&gt; 86.7 &lt;/td&gt;&lt;td&gt; 85.3 &lt;/td&gt;&lt;td&gt; 85.4 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   CoLA  &lt;/td&gt;&lt;td&gt; 33.6 &lt;/td&gt;&lt;td&gt; 43.0 &lt;/td&gt;&lt;td&gt; 41.2 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   SST-2 &lt;/td&gt;&lt;td&gt; 90.0 &lt;/td&gt;&lt;td&gt; 90.5 &lt;/td&gt;&lt;td&gt; 90.9 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QNLI  &lt;/td&gt;&lt;td&gt; 87.6 &lt;/td&gt;&lt;td&gt; 88.2 &lt;/td&gt;&lt;td&gt; 88.5 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MNLI  &lt;/td&gt;&lt;td&gt; 82.2 &lt;/td&gt;&lt;td&gt; 81.6 &lt;/td&gt;&lt;td&gt; 81.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QQP   &lt;/td&gt;&lt;td&gt; 88.5 &lt;/td&gt;&lt;td&gt; 87.1 &lt;/td&gt;&lt;td&gt; 87.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The fully finetuned GPT-2 model is trained for 4 epochs with a learning rate of 1e-4. The adapters are trained for 10 epochs with a learning rate of 1e-4.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt; BART &lt;/th&gt;&lt;th&gt; Full &lt;/th&gt;&lt;th&gt; Pfeiffer &lt;/th&gt;&lt;th&gt; Houlsby &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; RTE &lt;/td&gt;&lt;td&gt; 71.12 &lt;/td&gt;&lt;td&gt; 69.7 &lt;/td&gt;&lt;td&gt; 69.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MRPC  &lt;/td&gt;&lt;td&gt; 87.5&lt;/td&gt;&lt;td&gt; 86.8 &lt;/td&gt;&lt;td&gt; 88.2 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   STS-B &lt;/td&gt;&lt;td&gt; 89.0 &lt;/td&gt;&lt;td&gt; 88.1 &lt;/td&gt;&lt;td&gt; 88.3 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   CoLA  &lt;/td&gt;&lt;td&gt; 46.6 &lt;/td&gt;&lt;td&gt; 46.1 &lt;/td&gt;&lt;td&gt; 45.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   SST-2 &lt;/td&gt;&lt;td&gt; 92.7 &lt;/td&gt;&lt;td&gt; 93.7 &lt;/td&gt;&lt;td&gt; 93.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QNLI  &lt;/td&gt;&lt;td&gt; 91.6 &lt;/td&gt;&lt;td&gt; 92.2 &lt;/td&gt;&lt;td&gt; 93.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   MNLI  &lt;/td&gt;&lt;td&gt; 85.7 &lt;/td&gt;&lt;td&gt; 85.9 &lt;/td&gt;&lt;td&gt; 85.9 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;   QQP   &lt;/td&gt;&lt;td&gt; 89.3 &lt;/td&gt;&lt;td&gt; 88.4 &lt;/td&gt;&lt;td&gt; 88.6 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;The fully-finetuned BART model is trained for 3 epochs with a learning rate of 4e-5. The adapters are trained with early stopping for a maximum of 15 epochs with a learning rate of 1e-4.&lt;/p&gt;
&lt;p&gt;The results of the adapters are comparable to those of the fully finetuned model. On some tasks such as SST-2, the adapters achieve a higher score than the fully finetuned model for GPT-2 and BART. This matches the results of other models with adapters. In general, we can use adapters instead of fully finetuning the model without a deterioration in downstream task performance. &lt;/p&gt;
&lt;p&gt;Now we will take a look at the scores for sequence-to-sequence tasks. We train a GPT-2 model on the task proposed by &lt;a href="https://arxiv.org/abs/2004.10404"&gt;Chen et al. (2020)&lt;/a&gt;. This task requires the model to learn to generate entailing sentences w.r.t. the input. For example,  given a table containing the release dates for an album, the model is provided with a template and and has the objective to fill in the blanks.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Template: [ENT] was released in 6 [ENT] in [ENT].&lt;/p&gt;
&lt;p&gt;Gold sentence: Black Ice was released in 6 Countries in 2008.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It is not sufficient for the model to simply enter a number from the table; it needs to count all countries the album was released in, in 2008. We trained the GPT-2 model with small-sized GPT-2 vocabulary using maximum likelihood estimation. The results are given in the following table:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; BLEU-1 &lt;/th&gt;&lt;th&gt; BLEU-2 &lt;/th&gt;&lt;th&gt; BLEU-3 &lt;/th&gt;&lt;th&gt; Adv-Acc &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; GPT-2  &lt;/td&gt;&lt;td&gt; 48.8 &lt;/td&gt;&lt;td&gt; 27.1 &lt;/td&gt;&lt;td&gt; 12.6 &lt;/td&gt;&lt;td&gt; 62.3 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; GPT-2 + Pfeiffer &lt;/td&gt;&lt;td&gt; 46.3 &lt;/td&gt;&lt;td&gt; 24.8 &lt;/td&gt;&lt;td&gt; 11.2 &lt;/td&gt;&lt;td&gt; 60.1 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; GPT-2 + Houlsby &lt;/td&gt;&lt;td&gt; 45.5 &lt;/td&gt;&lt;td&gt; 23.9 &lt;/td&gt;&lt;td&gt; 10.5 &lt;/td&gt;&lt;td&gt; 59.7 &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;We observe that the models with adapters achieve a competitive results to full model fine-tuning. However, adapters have several advantages over fully finetuning, e.g., shorter training times, they require less memory to be stored, and they can easily be shared.&lt;/p&gt;
&lt;p&gt;To test the BART model on sequence-to-sequence tasks, we evaluated the model on the CNN/Daily Mail dataset (&lt;a href="https://arxiv.org/pdf/1506.03340.pdf"&gt;Hermann et al. (2015)&lt;/a&gt;; &lt;a href="https://arxiv.org/pdf/1704.04368.pdf"&gt;See et al., 2017&lt;/a&gt;) and the extreme summary dataset (XSum) dataset (&lt;a href="https://arxiv.org/pdf/1808.08745.pdf"&gt;Narayan et al., 2018&lt;/a&gt;). Both tasks have the objective to summarize newspaper articles. The main difference is that XSum requires the model to output short one sentence summaries. The results of the fully finetuned BART model and the adapters are as follows:&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; R1 &lt;/th&gt;&lt;th&gt; R2 &lt;/th&gt;&lt;th&gt; RL &lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; CNN/Daily mail &lt;/td&gt;&lt;td&gt; 44.16 &lt;/td&gt;&lt;td&gt; 21.28 &lt;/td&gt;&lt;td&gt; 40.90 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt;CNN/Daily mail + Pfeiffer &lt;/td&gt;&lt;td&gt; 43.40 &lt;/td&gt;&lt;td&gt; 20.86 &lt;/td&gt;&lt;td&gt; 30.66 &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;&lt;th&gt; R1 &lt;/th&gt;&lt;th&gt; R2 &lt;/th&gt;&lt;th&gt; RL &lt;/th&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum &lt;/td&gt;&lt;td&gt; 45.14 &lt;/td&gt;&lt;td&gt; 22.27 &lt;/td&gt;&lt;td&gt; 37.26 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum + Pfeiffer &lt;/td&gt;&lt;td&gt; 43.56 &lt;/td&gt;&lt;td&gt; 20.56 &lt;/td&gt;&lt;td&gt; 35.56 &lt;/td&gt;
&lt;/tr&gt;&lt;tr&gt;
&lt;td&gt; XSum + Houlsby &lt;/td&gt;&lt;td&gt;44.03 &lt;/td&gt;&lt;td&gt; 20.90 &lt;/td&gt;&lt;td&gt; 36.01 &lt;/td&gt;
&lt;/tr&gt;&lt;/table&gt;

&lt;p&gt;Similar to the GPT-2 model, the BART model achieves the highest score when it is fully fine-tuned. The models with adapters achieve slightly lower scores, further indicating that adapters might in general achieve slightly lower scores on sequence-to-sequence tasks. However, as previously stated, they have several other advantages.&lt;/p&gt;
&lt;p&gt;Version 2.0 of the AdapterHub framework opens up new possibilities such as experimenting with summarization and text generation tasks. Adapters for BART and GPT-2 enable us to tackle a wide variety of text generation tasks with adapters.&lt;/p&gt;
&lt;h2 id="hands-on-example-train-an-adapter-to-write-poems"&gt;Hands-on example: Train an adapter to write poems&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/Adapter-Hub/adapter-transformers/blob/master/notebooks/06_Text_Generation.ipynb"&gt;&lt;img alt="Open Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /&gt;&lt;/a&gt; &lt;br&gt;
To illustrate how we can use adapters for text generation, we provide a hands-on example for training adapters within GPT-2 on a poem dataset by &lt;a href="https://arxiv.org/pdf/2011.02686.pdf"&gt;Sheng et al. (2020)&lt;/a&gt; and let it create novel poems. The dataset contains poems from the Gutenberg project. The full code is available in the corresponding colab notebook linked above. If you have read the previous blog post, this might look very familiar. First, we need to add our adapters.  This is easily done with just a few lines of code:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;AutoModelForCausalLM&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Add a new adapter&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# Activate the adapter for training&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We have created the GPT-2 model and added an adapter with &lt;code&gt;add_adapter()&lt;/code&gt;. We only need to pass the name of the adapter &lt;code&gt;"poem"&lt;/code&gt;. After adding the new adapter, we call &lt;code&gt;train_adapter()&lt;/code&gt; and pass the name of our adapter. This does two things: Firstly, it freezes all parameters of the pre-trained model such that only the parameters of the adapter are updated during training. Secondly, it activates the adapter so that it is used in the forward pass. Next, we can train our model the same way we would without an adapter. In the end, we can save our trained adapter as follows.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path/to/adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We call &lt;code&gt;save_adapter()&lt;/code&gt; and provide the path to the directory where the adapter should be saved and the name of the adapter we want to save.
Now that we have our trained adapter, we want to generate some poems and see what it has learned. First, we need to create a model with a language modeling head and load our trained adapter. Then we activate the loaded adapter.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;transformers&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GPT2LMHeadModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GPT2Tokenizer&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;GPT2LMHeadModel&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_pretrained&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_adapter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;path/to/adapter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_active_adapters&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;poem&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With &lt;code&gt;load_adapter()&lt;/code&gt; we can load an adapter from the Hub by passing the name of the adapter specified in the hub. We can also load a local adapter by providing the path to the adapter. Then, we activate our adapter such that is used in the forward pass with &lt;code&gt;set_active_adapters()&lt;/code&gt;.
Finally, we can think of a beginning of a poem and let the model finish it. In this case, the model generates 5 poems for the given beginning. We can choose the one we like most from those. We choose to start our poem with "In the night". One of the poems our model generated was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the night;&lt;br /&gt;
when the stars shine on her head.&lt;br /&gt;
the mounds are deep,&lt;br /&gt;
and the water's dark,&lt;br /&gt;
and the water's cold&lt;br /&gt;
and with her hand,&lt;br /&gt;
with her lips,&lt;br /&gt;
in song and song,&lt;br /&gt;
the sound of the birds&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This can easily be applied to other datasets. Feel free to train your own adapter and upload it at the &lt;a href="https://adapterhub.ml/"&gt;Hub&lt;/a&gt; or browse the adapters trained by the community.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The new version 2.0 of the AdapterHub framework supports adapters for GPT-2 and BART. The support of these two models offers new possibilities in solving sequence to sequence tasks with adapters. To checkout AdapterHub and its other features, visit us on &lt;a href="https://github.com/Adapter-Hub/adapter-transformers"&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="acknowledgements"&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;We thank &lt;a href="https://www.behance.net/andrefellenberg"&gt;André Fellenberg&lt;/a&gt; for the BART illustration.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Bapna, A., Arivazhagan, N., &amp;amp; Firat, O. (2019). Simple, scalable adaptation for neural machine translation. EMNLP 2019, &lt;a href="https://www.aclweb.org/anthology/D19-1165.pdf"&gt;https://www.aclweb.org/anthology/D19-1165.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Chen, W., Chen, J., Su, Y., Chen, Z., &amp;amp; Wang, W. Y. (2020). Logical natural language generation from open-domain tables. ACL 2020, &lt;a href="https://www.aclweb.org/anthology/2020.acl-main.708.pdf"&gt;https://www.aclweb.org/anthology/2020.acl-main.708.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hermann, K. M., Kočiský, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., &amp;amp; Blunsom, P. (2015). Teaching machines to read and comprehend. NeurIPS 2015 &lt;a href="https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html."&gt;https://proceedings.neurips.cc/paper/2015/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., Laroussilhe, Q.D., Gesmundo, A., Attariyan, M., &amp;amp; Gelly, S. (2019). Parameter-Efficient Transfer Learning for NLP. ICML 2019, &lt;a href="http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf"&gt;http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Narayan, S., Cohen, S. B., &amp;amp; Lapata, M. (2018). Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. EMNLP 2018, &lt;a href="https://www.aclweb.org/anthology/D18-1206/"&gt;https://www.aclweb.org/anthology/D18-1206/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., &amp;amp; Gurevych, I. (2021). AdapterFusion: Non-Destructive Task Composition for Transfer Learning. EACL 2021, &lt;a href="https://www.aclweb.org/anthology/2021.eacl-main.39.pdf"&gt;https://www.aclweb.org/anthology/2021.eacl-main.39.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Philip†, J., Bérard, A., Gallé, M., Besacier, L. (2020). Monolingual Adapters for Zero-Shot Neural Machine Translation. EMNLP 2020, &lt;a href="https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf"&gt;https://www.aclweb.org/anthology/2020.emnlp-main.361.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;See, A., Liu, P. J., &amp;amp; Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. ACL 2017, &lt;a href="https://www.aclweb.org/anthology/P17-1099/"&gt;https://www.aclweb.org/anthology/P17-1099/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sheng, E., &amp;amp; Uthus, D. (2020). Investigating Societal Biases in a Poetry Composition System. Proceedings of the Second Workshop on Gender Bias in Natural Language Processing, &lt;a href="https://www.aclweb.org/anthology/2020.gebnlp-1.9/"&gt;https://www.aclweb.org/anthology/2020.gebnlp-1.9/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="citation"&gt;Citation&lt;/h2&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nc"&gt;@misc&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nl"&gt;sterz_2021&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;title&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Adapters for Generative and Seq2Seq Models in NLP}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="na"&gt;url&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp/}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;author&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Hannah Sterz and Clifton Poth and Andreas R\&amp;quot;uckl\&amp;#39;e and Jonas Pfeiffer}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;year&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{2021}&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
  &lt;span class="na"&gt;month&lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="s"&gt;{Apr}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;* equal contribution&lt;/p&gt;</content>
    <link href="https://adapterhub.ml/blog/2021/04/adapters-for-generative-and-seq2seq-models-in-nlp" rel="alternate"/>
    <summary>Adapters have proven to be an efficient alternative to fully finetung models. The version 2.0 of the AdapterHub framework includes adapters for the BART and GPT2 models.
</summary>
    <published>2021-04-29T00:00:00+00:00</published>
  </entry>
</feed>
